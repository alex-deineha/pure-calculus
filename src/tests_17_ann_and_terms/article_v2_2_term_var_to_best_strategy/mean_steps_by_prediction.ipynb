{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Import the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T05:42:29.508056800Z",
     "start_time": "2023-12-08T05:42:24.946215Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\__init__.py:177: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\voldo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, LSTM, Conv1D, GlobalAveragePooling1D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import BertConfig\n",
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import explained_variance_score\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T05:42:44.561117Z",
     "start_time": "2023-12-08T05:42:44.430335500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Available GPUs:\", physical_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T05:42:47.226718200Z",
     "start_time": "2023-12-08T05:42:47.204008700Z"
    }
   },
   "outputs": [],
   "source": [
    "sequence_len = 512\n",
    "batch_size = 64\n",
    "\n",
    "config = BertConfig(\n",
    "    vocab_size=9,\n",
    "    hidden_size=84,\n",
    "    num_hidden_layers=2,\n",
    "    num_attention_heads=6,\n",
    "    intermediate_size=64,\n",
    "    max_position_embeddings=sequence_len,\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load & Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T05:43:19.170728500Z",
     "start_time": "2023-12-08T05:43:19.108282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count all terms: 4282\n",
      "Count original terms: 4282\n",
      "\n",
      "number samples: 4282\n",
      "number samples only reducable: 4251\n",
      "\n",
      "max RI steps count: 386\n",
      "max LO steps count: 219\n",
      "Count TESTING samples: 4251\n"
     ]
    }
   ],
   "source": [
    "all_data = pd.read_csv(\"prepare_data/data_steps/steps_vars_term_str.csv\", delimiter=',')\n",
    "\n",
    "# leave only unique terms\n",
    "print(f\"Count all terms: {len(all_data)}\")\n",
    "all_data = all_data.drop_duplicates(subset=\"vars_terms\").reset_index(drop=True)\n",
    "print(f\"Count original terms: {len(all_data)}\\n\")\n",
    "\n",
    "# shuffle the dataset\n",
    "all_data = shuffle(all_data, random_state=33).reset_index(drop=True)\n",
    "\n",
    "# drop unreducable by LO or RI\n",
    "print(f\"number samples: {len(all_data)}\")\n",
    "all_data = all_data[[x_ != 1000 for x_ in all_data[\"RI_steps_num\"]]].reset_index(drop=True)\n",
    "all_data = all_data[[x_ != 1000 for x_ in all_data[\"LO_steps_num\"]]].reset_index(drop=True)\n",
    "print(f\"number samples only reducable: {len(all_data)}\\n\")\n",
    "\n",
    "print(f\"max RI steps count: {max(all_data['RI_steps_num'])}\")\n",
    "print(f\"max LO steps count: {max(all_data['LO_steps_num'])}\")\n",
    "\n",
    "x_test = all_data[\"vars_terms\"].tolist()\n",
    "# RI has fewer steps -> 1\n",
    "# Otherwise 0\n",
    "y_test = [1 if los > ris else 0 for los, ris in zip(all_data[\"LO_steps_num\"].tolist(), all_data[\"RI_steps_num\"].tolist())]\n",
    "y_lo_test = all_data[\"LO_steps_num\"].tolist()\n",
    "y_ri_test = all_data[\"RI_steps_num\"].tolist()\n",
    "\n",
    "print(f\"Count TESTING samples: {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T05:43:35.089861500Z",
     "start_time": "2023-12-08T05:43:34.616247200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count all terms: 45038\n",
      "Count original terms: 45038\n",
      "\n",
      "number samples: 45038\n",
      "number samples only reducable: 42912\n",
      "\n",
      "max RI steps count: 400\n",
      "max LO steps count: 308\n",
      "Count TRAINING samples: 42912\n"
     ]
    }
   ],
   "source": [
    "all_data = pd.read_csv(\"prepare_data/data_steps/steps_vars_term_str_train.csv\", delimiter=',')\n",
    "\n",
    "# leave only unique terms\n",
    "print(f\"Count all terms: {len(all_data)}\")\n",
    "all_data = all_data.drop_duplicates(subset=\"vars_terms\").reset_index(drop=True)\n",
    "print(f\"Count original terms: {len(all_data)}\\n\")\n",
    "\n",
    "# shuffle the dataset\n",
    "all_data = shuffle(all_data, random_state=33).reset_index(drop=True)\n",
    "\n",
    "# drop unreducable by LO or RI\n",
    "print(f\"number samples: {len(all_data)}\")\n",
    "all_data = all_data[[x_ != 1000 for x_ in all_data[\"RI_steps_num\"]]].reset_index(drop=True)\n",
    "all_data = all_data[[x_ != 1000 for x_ in all_data[\"LO_steps_num\"]]].reset_index(drop=True)\n",
    "print(f\"number samples only reducable: {len(all_data)}\\n\")\n",
    "\n",
    "print(f\"max RI steps count: {max(all_data['RI_steps_num'])}\")\n",
    "print(f\"max LO steps count: {max(all_data['LO_steps_num'])}\")\n",
    "\n",
    "x_train = all_data[\"vars_terms\"].tolist()\n",
    "# RI has fewer steps -> 1\n",
    "# Otherwise 0\n",
    "y_train = [1 if los > ris else 0 for los, ris in zip(all_data[\"LO_steps_num\"].tolist(), all_data[\"RI_steps_num\"].tolist())]\n",
    "y_lo_train = all_data[\"LO_steps_num\"].tolist()\n",
    "y_ri_train = all_data[\"RI_steps_num\"].tolist()\n",
    "\n",
    "print(f\"Count TRAINING samples: {len(y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "test_vars = set()\n",
    "for x_ in x_test:\n",
    "    test_vars.update(x_.replace(\"λ\", \" \").replace(\"(\", \" \").replace(\")\", \" \").replace(\".\", \" \").split(\" \"))\n",
    "    \n",
    "train_vars = set()\n",
    "for x_ in x_train:\n",
    "    train_vars.update(x_.replace(\"λ\", \" \").replace(\"(\", \" \").replace(\")\", \" \").replace(\".\", \" \").split(\" \"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T05:44:18.921347400Z",
     "start_time": "2023-12-08T05:44:16.670479700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x_1': 'X', 'y_1': 'Y', 'z_1': 'Z', 'a_1': 'A', 'b_1': 'B', 'c_1': 'C', 'd_1': 'D', 'e_1': 'E', 'j_1': 'J', 'i_1': 'I', 'n_1': 'N', 'm_1': 'M', 't_1': 'T', 'r_1': 'R', 'q_1': 'Q', 'w_1': 'W', 'u_1': 'U', 'o_1': 'O', 'p_1': 'P', 's_1': 'S', 'f_1': 'F', 'g_1': 'G', 'h_1': 'H', 'k_1': 'K', 'l_1': 'L', 'v_1': 'V', 'x_2': 'α', 'y_2': 'β', 'z_2': 'γ', 'a_2': 'δ', 'b_2': 'ε', 'c_2': 'ζ', 'd_2': 'η', 'e_2': 'θ', 'j_2': 'ι', 'i_2': 'κ', 'n_2': 'λ', 'm_2': 'μ', 't_2': 'ν', 'r_2': 'ξ', 'q_2': 'ο', 'w_2': 'π', 'u_2': 'ρ', 'o_2': 'σ', 'p_2': 'τ', 's_2': 'υ', 'f_2': 'φ', 'g_2': 'χ', 'h_2': 'ψ', 'k_2': 'ω', 'l_2': 'Α', 'v_2': 'Β', 'x_3': 'Γ', 'y_3': 'Δ', 'z_3': 'Ε', 'a_3': 'Ζ', 'b_3': 'Η', 'c_3': 'Θ', 'd_3': 'Ι', 'e_3': 'Κ', 'j_3': 'Λ', 'i_3': 'Μ', 'n_3': 'Ν', 'm_3': 'Ξ', 't_3': 'Ο', 'r_3': 'Π', 'q_3': 'Ρ', 'w_3': 'Σ', 'u_3': 'Τ', 'o_3': 'Υ', 'p_3': 'Φ', 's_3': 'Χ', 'f_3': 'Ψ', 'g_3': 'Ω', 'h_3': '1', 'k_3': '2', 'l_3': '3', 'v_3': '4', 'x_4': '5', 'y_4': '6', 'z_4': '7', 'a_4': '8', 'b_4': '9', 'c_4': '0', 'e_5': 'e', 'k_4': 'k', 's_6': 's', 'c_7': 'c', 'e_8': 'e', 'a_5': 'a', 'b_7': 'b', 'e_6': 'e', 'n_4': 'n', 'p_8': 'p', 'y_7': 'y', 'x_6': 'x', 'y_8': 'y', 'c_8': 'c', 'k_6': 'k', 'u_8': 'u', 's_5': 's', 'w_5': 'w', 'd_4': 'd', 'i_5': 'i', 'w_8': 'w', 'f_8': 'f', 's_4': 's', 'b_5': 'b', 'o_8': 'o', 'y_5': 'y', 'k_8': 'k', 'h_5': 'h', 'h_8': 'h', 'l_7': 'l', 'j_8': 'j', 'w_4': 'w', 'l_4': 'l', 'p_7': 'p', 'c_6': 'c', 'o_4': 'o', 'm_6': 'm', 't_5': 't', 'm_7': 'm', 'a_7': 'a', 'v_4': 'v', 'q_8': 'q', 'd_6': 'd', 'r_7': 'r', 't_7': 't', 'a_8': 'a', 'n_6': 'n', 'g_6': 'g', 'v_5': 'v', 'r_5': 'r', 'f_4': 'f', 'e_7': 'e', 'j_7': 'j', 'd_5': 'd', 'u_7': 'u', 'r_8': 'r', 'k_7': 'k', 't_6': 't', 'p_6': 'p', 's_7': 's', 'r_4': 'r', 'o_7': 'o', 'j_6': 'j', 'c_5': 'c', 'y_6': 'y', 'u_6': 'u', 'k_5': 'k', 'r_6': 'r', 'o_6': 'o', 'j_4': 'j', 'm_8': 'm', 'n_5': 'n', 'i_7': 'i', 'i_4': 'i', 'u_4': 'u', 'x_8': 'x', 'm_5': 'm', 'j_5': 'j', 'v_6': 'v', 'p_5': 'p', 'z_8': 'z', 'n_8': 'n', 'q_5': 'q', 'i_8': 'i', 't_4': 't', 'i_6': 'i', 'l_6': 'l', 'v_7': 'v', 'x_5': 'x', 'a_6': 'a', 'h_6': 'h', 'q_7': 'q', 'm_4': 'm', 'f_5': 'f', 's_8': 's', 'h_4': 'h', 'w_7': 'w', 'w_6': 'w', 'f_7': 'f', 'l_5': 'l', 'z_7': 'z', 't_8': 't', 'z_5': 'z', 'g_7': 'g', 'o_5': 'o', 'n_7': 'n', 'g_8': 'g', 'g_5': 'g', 'd_7': 'd', 'u_5': 'u', 'q_4': 'q', 'd_8': 'd', 'x_7': 'x', 'q_6': 'q', 'z_6': 'z', 'b_8': 'b', 'f_6': 'f', 'h_7': 'h', 'b_6': 'b', 'e_4': 'e', 'p_4': 'p', 'g_4': 'g', 'w_14': 'w', 'j_28': 'j', 'e_11': 'e', 'k_19': 'k', 'm_15': 'm', 'w_11': 'w', 'z_19': 'z', 'd_21': 'd', 'q_18': 'q', 'e_16': 'e', 'k_27': 'k', 'c_26': 'c', 'i_12': 'i', 'b_13': 'b', 'i_9': 'i', 'a_26': 'a', 'z_28': 'z', 'k_21': 'k', 'u_24': 'u', 'u_10': 'u', 'f_22': 'f', 'm_22': 'm', 'm_11': 'm', 'e_15': 'e', 'j_11': 'j', 'x_16': 'x', 'r_21': 'r', 't_27': 't', 'j_24': 'j', 'r_14': 'r', 'l_17': 'l', 'd_24': 'd', 'j_17': 'j', 'z_13': 'z', 's_27': 's', 'm_23': 'm', 'p_23': 'p', 'w_20': 'w', 'n_19': 'n', 'l_12': 'l', 'o_25': 'o', 'y_15': 'y', 'z_21': 'z', 'p_21': 'p', 'k_12': 'k', 'y_27': 'y', 'u_22': 'u', 'j_14': 'j', 'o_24': 'o', 'v_26': 'v', 'q_13': 'q', 'c_18': 'c', 'i_20': 'i', 'm_19': 'm', 'z_10': 'z', 'l_13': 'l', 'b_19': 'b', 'v_19': 'v', 'w_25': 'w', 'v_23': 'v', 'o_16': 'o', 'j_20': 'j', 'i_10': 'i', 'd_14': 'd', 'l_21': 'l', 'c_23': 'c', 'k_25': 'k', 't_18': 't', 'c_12': 'c', 'h_15': 'h', 'm_14': 'm', 'r_12': 'r', 'a_10': 'a', 'v_22': 'v', 'e_17': 'e', 's_24': 's', 'u_26': 'u', 'r_22': 'r', 'i_24': 'i', 'r_13': 'r', 'e_26': 'e', 'l_24': 'l', 'x_9': 'x', 'h_9': 'h', 'x_21': 'x', 'e_12': 'e', 'x_19': 'x', 'y_21': 'y', 'l_15': 'l', 'm_18': 'm', 'q_28': 'q', 'v_17': 'v', 'f_17': 'f', 'y_18': 'y', 'd_28': 'd', 'l_26': 'l', 'g_17': 'g', 's_10': 's', 'u_11': 'u', 'd_26': 'd', 'w_28': 'w', 'r_28': 'r', 'u_23': 'u', 'v_12': 'v', 't_22': 't', 'y_26': 'y', 'q_12': 'q', 'a_20': 'a', 'v_14': 'v', 'h_19': 'h', 'p_18': 'p', 'w_27': 'w', 'x_18': 'x', 'q_19': 'q', 'h_24': 'h', 't_24': 't', 'f_27': 'f', 'm_17': 'm', 'm_28': 'm', 'c_9': 'c', 'u_21': 'u', 'h_17': 'h', 'h_22': 'h', 'v_13': 'v', 'd_13': 'd', 'w_26': 'w', 'n_13': 'n', 'e_13': 'e', 'x_20': 'x', 's_22': 's', 'n_26': 'n', 'p_17': 'p', 'w_24': 'w', 'a_16': 'a', 'p_10': 'p', 'q_16': 'q', 'd_17': 'd', 'h_23': 'h', 'x_26': 'x', 's_25': 's', 'k_11': 'k', 'f_20': 'f', 'i_25': 'i', 'a_9': 'a', 'c_19': 'c', 'l_18': 'l', 'm_26': 'm', 'm_27': 'm', 'q_20': 'q', 'd_22': 'd', 'c_11': 'c', 'y_23': 'y', 'n_23': 'n', 'i_22': 'i', 'p_11': 'p', 'y_25': 'y', 'a_22': 'a', 'j_26': 'j', 'd_19': 'd', 'z_14': 'z', 'l_8': 'l', 'u_28': 'u', 'i_16': 'i', 'x_12': 'x', 's_9': 's', 't_19': 't', 'w_9': 'w', 'i_27': 'i', 'q_9': 'q', 'y_19': 'y', 'r_18': 'r', 'm_16': 'm', 's_16': 's', 'g_22': 'g', 'f_26': 'f', 'c_15': 'c', 'a_11': 'a', 'g_13': 'g', 'd_18': 'd', 'v_10': 'v', 'z_26': 'z', 'x_11': 'x', 'i_17': 'i', 'h_26': 'h', 'f_18': 'f', 'k_16': 'k', 'z_20': 'z', 'n_28': 'n', 'f_14': 'f', 'l_20': 'l', 'h_27': 'h', 'j_10': 'j', 'x_28': 'x', 'c_27': 'c', 'j_22': 'j', 'b_12': 'b', 'p_25': 'p', 'o_19': 'o', 'o_11': 'o', 'b_27': 'b', 'l_22': 'l', 'a_15': 'a', 'w_19': 'w', 'g_9': 'g', 'a_28': 'a', 'i_11': 'i', 'i_19': 'i', 'o_10': 'o', 'f_9': 'f', 'n_14': 'n', 'q_24': 'q', 'z_18': 'z', 'l_27': 'l', 'y_10': 'y', 'p_15': 'p', 'n_24': 'n', 'f_16': 'f', 'y_16': 'y', 't_9': 't', 'c_16': 'c', 's_26': 's', 'i_13': 'i', 'k_15': 'k', 'a_23': 'a', 'a_19': 'a', 'l_23': 'l', 't_11': 't', 't_21': 't', 'l_19': 'l', 'x_24': 'x', 'q_10': 'q', 'v_11': 'v', 'y_11': 'y', 'e_27': 'e', 'r_10': 'r', 'd_20': 'd', 'r_27': 'r', 'i_14': 'i', 'r_26': 'r', 'i_21': 'i', 'f_10': 'f', 'z_16': 'z', 'r_19': 'r', 'y_24': 'y', 'k_13': 'k', 'k_17': 'k', 'w_10': 'w', 'h_25': 'h', 'u_19': 'u', 'h_10': 'h', 'v_27': 'v', 'n_17': 'n', 'b_22': 'b', 'z_12': 'z', 'n_27': 'n', 's_14': 's', 'q_15': 'q', 'f_12': 'f', 'c_24': 'c', 'j_13': 'j', 'y_13': 'y', 's_19': 's', 'j_25': 'j', 'q_26': 'q', 'v_16': 'v', 's_23': 's', 'g_20': 'g', 'd_23': 'd', 't_13': 't', 'x_27': 'x', 'j_27': 'j', 's_18': 's', 'f_13': 'f', 's_21': 's', 't_10': 't', 'a_17': 'a', 'j_21': 'j', 'z_15': 'z', 'g_24': 'g', 'm_25': 'm', 'b_24': 'b', 'g_25': 'g', 'i_18': 'i', 'o_23': 'o', 'u_27': 'u', 'p_20': 'p', 'a_13': 'a', 'a_21': 'a', 'k_14': 'k', 's_20': 's', 'o_26': 'o', 'c_22': 'c', 'u_13': 'u', 'q_17': 'q', 'o_9': 'o', 'h_21': 'h', 'p_14': 'p', 'o_13': 'o', 'z_11': 'z', 'g_12': 'g', 'h_11': 'h', 'a_18': 'a', 'c_20': 'c', 'b_10': 'b', 'g_11': 'g', 'l_14': 'l', 'm_13': 'm', 'a_14': 'a', 't_25': 't', 'h_20': 'h', 'r_24': 'r', 't_12': 't', 'k_26': 'k', 'j_19': 'j', 'p_19': 'p', 'w_18': 'w', 'r_20': 'r', 't_14': 't', 'd_10': 'd', 'b_20': 'b', 'p_12': 'p', 'x_22': 'x', 'm_9': 'm', 'i_15': 'i', 'j_18': 'j', 'j_9': 'j', 'z_27': 'z', 'k_9': 'k', 'w_17': 'w', 'y_22': 'y', 'g_21': 'g', 'o_12': 'o', 'h_14': 'h', 'h_18': 'h', 'z_23': 'z', 'z_22': 'z', 'i_23': 'i', 'z_9': 'z', 'b_17': 'b', 'k_20': 'k', 'b_25': 'b', 'g_18': 'g', 'v_8': 'v', 's_11': 's', 'g_23': 'g', 'b_9': 'b', 'u_25': 'u', 'z_17': 'z', 'r_17': 'r', 'j_15': 'j', 'q_21': 'q', 'e_25': 'e', 'n_10': 'n', 'd_25': 'd', 'u_14': 'u', 'e_10': 'e', 'x_14': 'x', 'u_12': 'u', 'o_14': 'o', 's_15': 's', 'b_26': 'b', 'w_21': 'w', 'x_17': 'x', 'e_21': 'e', 'o_28': 'o', 'c_13': 'c', 'q_27': 'q', 'o_22': 'o', 'y_20': 'y', 'n_22': 'n', 'm_24': 'm', 'd_15': 'd', 'n_20': 'n', 'g_26': 'g', 'w_23': 'w', 'u_17': 'u', 'o_27': 'o', 'e_28': 'e', 'l_10': 'l', 'd_9': 'd', 'z_25': 'z', 'v_15': 'v', 'e_20': 'e', 'a_12': 'a', 'c_14': 'c', 'l_25': 'l', 't_20': 't', 'b_23': 'b', 'a_25': 'a', 'l_16': 'l', 'u_16': 'u', 'f_23': 'f', 't_26': 't', 'y_28': 'y', 'o_17': 'o', 'l_9': 'l', 's_13': 's', 'v_20': 'v', 's_17': 's', 'w_13': 'w', 'y_14': 'y', 'd_16': 'd', 'p_16': 'p', 'r_25': 'r', 'c_21': 'c', 'r_23': 'r', 'v_21': 'v', 'z_24': 'z', 'p_27': 'p', 'u_15': 'u', 'x_15': 'x', 'g_10': 'g', 'x_23': 'x', 'r_11': 'r', 'f_15': 'f', 'g_16': 'g', 'j_23': 'j', 'h_13': 'h', 'a_24': 'a', 'x_13': 'x', 'w_16': 'w', 'c_28': 'c', 'j_12': 'j', 'r_16': 'r', 'c_17': 'c', 'e_19': 'e', 'a_27': 'a', 'f_25': 'f', 'b_15': 'b', 'e_22': 'e', 't_28': 't', 'y_17': 'y', 'b_16': 'b', 'f_24': 'f', 'p_13': 'p', 'g_14': 'g', 'd_12': 'd', 'k_24': 'k', 'm_21': 'm', 'c_10': 'c', 'm_10': 'm', 'o_15': 'o', 'n_11': 'n', 'o_21': 'o', 'm_12': 'm', 't_17': 't', 'e_18': 'e', 's_12': 's', 'b_21': 'b', 'v_9': 'v', 'g_19': 'g', 'i_26': 'i', 'q_11': 'q', 'p_24': 'p', 'e_9': 'e', 'o_20': 'o', 'q_23': 'q', 'f_11': 'f', 'p_22': 'p', 't_16': 't', 'o_18': 'o', 'e_24': 'e', 'p_26': 'p', 't_23': 't', 'i_28': 'i', 'c_25': 'c', 'b_18': 'b', 'w_12': 'w', 'w_15': 'w', 'x_10': 'x', 'b_28': 'b', 'h_12': 'h', 'v_18': 'v', 'x_25': 'x', 'r_9': 'r', 'v_24': 'v', 'e_14': 'e', 'q_14': 'q', 'p_9': 'p', 'y_12': 'y', 'n_18': 'n', 'j_16': 'j', 'q_22': 'q', 'm_20': 'm', 'g_15': 'g', 'n_15': 'n', 'u_9': 'u', 'v_25': 'v', 'n_16': 'n', 'h_16': 'h', 'k_10': 'k', 'k_22': 'k', 'n_12': 'n', 'u_18': 'u', 'u_20': 'u', 'n_21': 'n', 'e_23': 'e', 'd_11': 'd', 'f_21': 'f', 'k_23': 'k', 'g_27': 'g', 'n_9': 'n', 'b_14': 'b', 'd_27': 'd', 't_15': 't', 'b_11': 'b', 'q_25': 'q', 'r_15': 'r', 'k_18': 'k', 'l_11': 'l', 'w_22': 'w', 'f_19': 'f', 'y_9': 'y', 'n_25': 'n'}\n"
     ]
    }
   ],
   "source": [
    "vars_to_sym_dict = dict()\n",
    "\n",
    "list_vars = \"x y z a b c d e j i n m t r q w u o p s f g h k l v\".split()\n",
    "greek_small = \"α β γ δ ε ζ η θ ι κ λ μ ν ξ ο π ρ σ τ υ φ χ ψ ω Α Β\".split()\n",
    "greek_big = \"Γ Δ Ε Ζ Η Θ Ι Κ Λ Μ Ν Ξ Ο Π Ρ Σ Τ Υ Φ Χ Ψ Ω 1 2 3 4\".split()\n",
    "            \n",
    "for kv_ in list_vars:\n",
    "    vars_to_sym_dict[kv_] = kv_\n",
    "\n",
    "for key_, val_ in zip(list_vars, \"X Y Z A B C D E J I N M T R Q W U O P S F G H K L V\".split()):\n",
    "    vars_to_sym_dict[key_ + \"_1\"] = val_\n",
    "    \n",
    "for key_, val_ in zip(list_vars, greek_small):\n",
    "    vars_to_sym_dict[key_ + \"_2\"] = val_\n",
    "    \n",
    "for key_, val_ in zip(list_vars, greek_big):\n",
    "    vars_to_sym_dict[key_ + \"_3\"] = val_\n",
    "    \n",
    "for key_, val_ in zip(list_vars[:6], \"5 6 7 8 9 0\".split()):\n",
    "    vars_to_sym_dict[key_ + \"_4\"] = val_\n",
    "\n",
    "for x_ in test_vars:\n",
    "    if x_ and x_ not in vars_to_sym_dict:\n",
    "        vars_to_sym_dict[x_] = x_[0]\n",
    "        \n",
    "for x_ in train_vars:\n",
    "    if x_ and x_ not in vars_to_sym_dict:\n",
    "        vars_to_sym_dict[x_] = x_[0]\n",
    "\n",
    "for x_ in list_vars:\n",
    "    del vars_to_sym_dict[x_]\n",
    "\n",
    "print(vars_to_sym_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T05:45:08.935454500Z",
     "start_time": "2023-12-08T05:45:08.891994800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T05:45:12.563495100Z",
     "start_time": "2023-12-08T05:45:12.515421100Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"tokenizer_data\", max_len=sequence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T05:46:31.208724400Z",
     "start_time": "2023-12-08T05:46:05.103398800Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train_prep = []\n",
    "for x_ in x_train:\n",
    "    x_ = x_.replace(\"λ\", \"@\")\n",
    "    for key_, val_ in vars_to_sym_dict.items():\n",
    "        x_ = x_.replace(key_, val_)\n",
    "    x_ = x_.replace(\".\", \"\").replace(\" \", \"\")\n",
    "    x_train_prep.append(x_)\n",
    "    \n",
    "x_test_prep = []\n",
    "for x_ in x_test:\n",
    "    x_ = x_.replace(\"λ\", \"@\")\n",
    "    for key_, val_ in vars_to_sym_dict.items():\n",
    "        x_ = x_.replace(key_, val_)\n",
    "    x_ = x_.replace(\".\", \"\").replace(\" \", \"\")\n",
    "    x_test_prep.append(x_)\n",
    "\n",
    "train_df = pd.DataFrame({\n",
    "    \"term_str\": x_train_prep, \"is_ri_best\": y_train,\n",
    "    \"lo_steps\": y_lo_train, \"ri_steps\": y_ri_train,\n",
    "    \"sample_weights\": [(abs(ri_ - lo_) / (max(ri_, lo_) + 0.001)) + 0.001 for ri_, lo_ in zip(y_ri_train, y_lo_train)],\n",
    "})\n",
    "test_df = pd.DataFrame({\n",
    "    \"term_str\": x_test_prep, \"is_ri_best\": y_test,\n",
    "    \"lo_steps\": y_lo_test, \"ri_steps\": y_ri_test,\n",
    "    \"sample_weights\": [(abs(ri_ - lo_) / (max(ri_, lo_) + 0.001)) + 0.001 for ri_, lo_ in zip(y_ri_test, y_lo_test)],\n",
    "})\n",
    "\n",
    "def preprocess(example):\n",
    "    # Tokenize the prompt\n",
    "    tokenized_texts = tokenizer(example['term_str'].to_list(), truncation=True, padding='max_length', max_length=sequence_len, return_tensors=\"tf\")\n",
    "    labels = tf.convert_to_tensor(example[\"is_ri_best\"])\n",
    "    return tokenized_texts, labels\n",
    "\n",
    "\n",
    "tokenized_train_data = preprocess(train_df)\n",
    "tokenized_test_data = preprocess(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T05:46:31.252413300Z",
     "start_time": "2023-12-08T05:46:31.211034300Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(tokenized_train_data[0]), tokenized_train_data[1])).batch(batch_size)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(tokenized_test_data[0]), tokenized_test_data[1])).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T05:46:31.277756300Z",
     "start_time": "2023-12-08T05:46:31.243299700Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(actual_labels, predicted_labels):\n",
    "    correct_predictions = sum(1 for actual, predicted in zip(actual_labels, predicted_labels) if actual == predicted)\n",
    "    total_predictions = len(actual_labels)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, class_labels, normalize=False, title=None, cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function plots the confusion matrix of a classification model.\n",
    "\n",
    "    Args:\n",
    "        y_true (numpy.ndarray): The ground truth labels.\n",
    "        y_pred (numpy.ndarray): The predicted labels.\n",
    "        classes (list): The list of class labels.\n",
    "        class_labels: The list of class names.\n",
    "        normalize (bool, optional): Whether to normalize the confusion matrix. Defaults to False.\n",
    "        title (str, optional): The title of the plot. Defaults to None.\n",
    "        cmap (matplotlib.colors.Colormap, optional): The colormap to use for the plot. Defaults to plt.cm.Blues.\n",
    "    \"\"\"\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=classes).astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.set_xticks(np.arange(len(classes)))\n",
    "    ax.set_yticks(np.arange(len(classes)))\n",
    "    ax.set_xticklabels(class_labels)\n",
    "    ax.set_yticklabels(class_labels)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    ax.set_title(title)\n",
    "    fig.colorbar(im)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(len(classes)):\n",
    "        for j in range(len(classes)):\n",
    "            ij = float(cm[i, j])\n",
    "            ax.text(j, i, f\"{ij:.2f}\", ha='center', va='center', color='white' if ij > thresh else 'black')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# The best & worst possible steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T05:47:02.083487500Z",
     "start_time": "2023-12-08T05:47:02.044221300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test BEST steps: avg=15.143, sum=64372\n",
      "Train BEST steps: avg=13.082, sum=561374\n",
      "\n",
      "Test WORST steps: avg=23.707, sum=100780\n",
      "Train WORST steps: avg=23.989, sum=1029427\n",
      "\n",
      "Test WORST LO steps: avg=16.635, sum=70714\n",
      "Train WORST LO steps: avg=15.328, sum=657760\n",
      "\n",
      "Test WORST RI steps: avg=22.215, sum=94438\n",
      "Train WORST RI steps: avg=21.743, sum=933041\n"
     ]
    }
   ],
   "source": [
    "y_test_best_sum = 0\n",
    "for lo_, ri_ in zip(y_lo_test, y_ri_test):\n",
    "    y_test_best_sum += lo_ if lo_ <= ri_ else ri_\n",
    "y_test_best_avg = y_test_best_sum / len(y_lo_test)\n",
    "\n",
    "y_train_best_sum = 0\n",
    "for lo_, ri_ in zip(y_lo_train, y_ri_train):\n",
    "    y_train_best_sum += lo_ if lo_ <= ri_ else ri_\n",
    "y_train_best_avg = y_train_best_sum / len(y_lo_train)\n",
    "\n",
    "\n",
    "y_test_worst_sum = 0\n",
    "for lo_, ri_ in zip(y_lo_test, y_ri_test):\n",
    "    y_test_worst_sum += lo_ if lo_ > ri_ else ri_\n",
    "y_test_worst_avg = y_test_worst_sum / len(y_lo_test)\n",
    "\n",
    "y_train_worst_sum = 0\n",
    "for lo_, ri_ in zip(y_lo_train, y_ri_train):\n",
    "    y_train_worst_sum += lo_ if lo_ > ri_ else ri_\n",
    "y_train_worst_avg = y_train_worst_sum / len(y_lo_train)\n",
    "\n",
    "\n",
    "y_test_worst_LO_sum = np.sum(y_lo_test)\n",
    "y_test_worst_LO_avg = np.mean(y_lo_test)\n",
    "\n",
    "y_train_worst_LO_sum = np.sum(y_lo_train)\n",
    "y_train_worst_LO_avg = np.mean(y_lo_train)\n",
    "\n",
    "\n",
    "y_test_worst_RI_sum = np.sum(y_ri_test)\n",
    "y_test_worst_RI_avg = np.mean(y_ri_test)\n",
    "\n",
    "y_train_worst_RI_sum = np.sum(y_ri_train)\n",
    "y_train_worst_RI_avg = np.mean(y_ri_train)\n",
    "\n",
    "\n",
    "print(f\"Test BEST steps: avg={y_test_best_avg:.3f}, sum={y_test_best_sum}\")\n",
    "print(f\"Train BEST steps: avg={y_train_best_avg:.3f}, sum={y_train_best_sum}\\n\")\n",
    "\n",
    "print(f\"Test WORST steps: avg={y_test_worst_avg:.3f}, sum={y_test_worst_sum}\")\n",
    "print(f\"Train WORST steps: avg={y_train_worst_avg:.3f}, sum={y_train_worst_sum}\\n\")\n",
    "\n",
    "print(f\"Test WORST LO steps: avg={y_test_worst_LO_avg:.3f}, sum={y_test_worst_LO_sum}\")\n",
    "print(f\"Train WORST LO steps: avg={y_train_worst_LO_avg:.3f}, sum={y_train_worst_LO_sum}\\n\")\n",
    "\n",
    "print(f\"Test WORST RI steps: avg={y_test_worst_RI_avg:.3f}, sum={y_test_worst_RI_sum}\")\n",
    "print(f\"Train WORST RI steps: avg={y_train_worst_RI_avg:.3f}, sum={y_train_worst_RI_sum}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T05:47:05.776286200Z",
     "start_time": "2023-12-08T05:47:05.732661300Z"
    }
   },
   "outputs": [],
   "source": [
    "def calc_steps_accuracy(y_predictions, y_lo_steps, y_ri_steps, threshold=0.5):\n",
    "    y_steps_sum = 0\n",
    "    for lo_, ri_, pred_ in zip(y_lo_steps, y_ri_steps, y_predictions):\n",
    "        y_steps_sum += lo_ if pred_ < threshold else ri_\n",
    "    y_steps_avg = y_steps_sum / len(y_lo_steps)\n",
    "    return y_steps_sum, y_steps_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Test 90 epochs trained model on all terms with vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T05:50:19.416450900Z",
     "start_time": "2023-12-08T05:49:32.878860700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 6s 52ms/step\n",
      "671/671 [==============================] - 35s 52ms/step\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "from transformers import TFBertModel\n",
    "\n",
    "with open(\"./fine_models/model_vars_ri.json\", \"r\") as file:\n",
    "    loaded_model_json = file.read()\n",
    "\n",
    "model = tf.keras.models.model_from_json(loaded_model_json, custom_objects={\"TFBertModel\": TFBertModel})\n",
    "model.load_weights('./fine_models/model_vars_ri.h5')\n",
    "\n",
    "\n",
    "y_test_pred = model.predict(test_dataset)\n",
    "y_train_pred = model.predict(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T05:50:37.407717800Z",
     "start_time": "2023-12-08T05:50:37.128687Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test BEST steps: avg=15.143, sum=64372\n",
      "Train BEST steps: avg=13.082, sum=561374\n",
      "\n",
      "0.5th\n",
      "Test steps: avg=17.424, sum=74071\n",
      "Train steps: avg=13.343, sum=572562\n",
      "\n",
      "0.35th\n",
      "Test steps: avg=17.594, sum=74793\n",
      "Train steps: avg=13.402, sum=575114\n",
      "\n",
      "0.15th\n",
      "Test steps: avg=18.511, sum=78690\n",
      "Train steps: avg=13.620, sum=584445\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test BEST steps: avg={y_test_best_avg:.3f}, sum={y_test_best_sum}\")\n",
    "print(f\"Train BEST steps: avg={y_train_best_avg:.3f}, sum={y_train_best_sum}\\n\")\n",
    "\n",
    "print(f\"0.5th\")\n",
    "y_test_sum_05, y_test_avg_05 = calc_steps_accuracy(y_test_pred, y_lo_test, y_ri_test, threshold=0.5)\n",
    "y_train_sum_05, y_train_avg_05 = calc_steps_accuracy(y_train_pred, y_lo_train, y_ri_train, threshold=0.5)\n",
    "print(f\"Test steps: avg={y_test_avg_05:.3f}, sum={y_test_sum_05}\")\n",
    "print(f\"Train steps: avg={y_train_avg_05:.3f}, sum={y_train_sum_05}\\n\")\n",
    "\n",
    "print(f\"0.35th\")\n",
    "y_test_sum_05, y_test_avg_05 = calc_steps_accuracy(y_test_pred, y_lo_test, y_ri_test, threshold=0.35)\n",
    "y_train_sum_05, y_train_avg_05 = calc_steps_accuracy(y_train_pred, y_lo_train, y_ri_train, threshold=0.35)\n",
    "print(f\"Test steps: avg={y_test_avg_05:.3f}, sum={y_test_sum_05}\")\n",
    "print(f\"Train steps: avg={y_train_avg_05:.3f}, sum={y_train_sum_05}\\n\")\n",
    "\n",
    "print(f\"0.15th\")\n",
    "y_test_sum_05, y_test_avg_05 = calc_steps_accuracy(y_test_pred, y_lo_test, y_ri_test, threshold=0.15)\n",
    "y_train_sum_05, y_train_avg_05 = calc_steps_accuracy(y_train_pred, y_lo_train, y_ri_train, threshold=0.15)\n",
    "print(f\"Test steps: avg={y_test_avg_05:.3f}, sum={y_test_sum_05}\")\n",
    "print(f\"Train steps: avg={y_train_avg_05:.3f}, sum={y_train_sum_05}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Test 90 epochs trained, best f1 model on all terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T05:51:58.572965200Z",
     "start_time": "2023-12-08T05:51:17.841658500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 5s 52ms/step\n",
      "671/671 [==============================] - 35s 52ms/step\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "from transformers import TFBertModel\n",
    "\n",
    "with open(\"./fine_models/model_vars_ri.json\", \"r\") as file:\n",
    "    loaded_model_json = file.read()\n",
    "\n",
    "model = tf.keras.models.model_from_json(loaded_model_json, custom_objects={\"TFBertModel\": TFBertModel})\n",
    "model.load_weights('./fine_models/model_vars_ri_f1.h5')\n",
    "\n",
    "\n",
    "y_test_pred = model.predict(test_dataset)\n",
    "y_train_pred = model.predict(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T05:51:58.853992800Z",
     "start_time": "2023-12-08T05:51:58.575974900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test BEST steps: avg=15.143, sum=64372\n",
      "Train BEST steps: avg=13.082, sum=561374\n",
      "\n",
      "0.5th\n",
      "Test steps: avg=17.713, sum=75298\n",
      "Train steps: avg=13.417, sum=575766\n",
      "\n",
      "0.35th\n",
      "Test steps: avg=17.869, sum=75963\n",
      "Train steps: avg=13.489, sum=578834\n",
      "\n",
      "0.15th\n",
      "Test steps: avg=18.382, sum=78143\n",
      "Train steps: avg=13.769, sum=590870\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test BEST steps: avg={y_test_best_avg:.3f}, sum={y_test_best_sum}\")\n",
    "print(f\"Train BEST steps: avg={y_train_best_avg:.3f}, sum={y_train_best_sum}\\n\")\n",
    "\n",
    "print(f\"0.5th\")\n",
    "y_test_sum_05, y_test_avg_05 = calc_steps_accuracy(y_test_pred, y_lo_test, y_ri_test, threshold=0.5)\n",
    "y_train_sum_05, y_train_avg_05 = calc_steps_accuracy(y_train_pred, y_lo_train, y_ri_train, threshold=0.5)\n",
    "print(f\"Test steps: avg={y_test_avg_05:.3f}, sum={y_test_sum_05}\")\n",
    "print(f\"Train steps: avg={y_train_avg_05:.3f}, sum={y_train_sum_05}\\n\")\n",
    "\n",
    "print(f\"0.35th\")\n",
    "y_test_sum_05, y_test_avg_05 = calc_steps_accuracy(y_test_pred, y_lo_test, y_ri_test, threshold=0.35)\n",
    "y_train_sum_05, y_train_avg_05 = calc_steps_accuracy(y_train_pred, y_lo_train, y_ri_train, threshold=0.35)\n",
    "print(f\"Test steps: avg={y_test_avg_05:.3f}, sum={y_test_sum_05}\")\n",
    "print(f\"Train steps: avg={y_train_avg_05:.3f}, sum={y_train_sum_05}\\n\")\n",
    "\n",
    "print(f\"0.15th\")\n",
    "y_test_sum_05, y_test_avg_05 = calc_steps_accuracy(y_test_pred, y_lo_test, y_ri_test, threshold=0.15)\n",
    "y_train_sum_05, y_train_avg_05 = calc_steps_accuracy(y_train_pred, y_lo_train, y_ri_train, threshold=0.15)\n",
    "print(f\"Test steps: avg={y_test_avg_05:.3f}, sum={y_test_sum_05}\")\n",
    "print(f\"Train steps: avg={y_train_avg_05:.3f}, sum={y_train_sum_05}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
