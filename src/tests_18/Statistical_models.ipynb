{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Apply statistical models to solve different problems:\n",
    "\n",
    "1. Test and validate assumptions for regression models, including the impact of multicollinearity in regression.\n",
    "2. Use a regression model to predict numerical or categorical values.\n",
    "\n",
    "Apply hypothesis testing:\n",
    "1. Develop and test hypotheses to check the validity of various claims about real world events."
   ],
   "metadata": {
    "id": "tLgSuV4Lc8c1"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Regression Models:**\n",
    "Regression models are statistical methods used to quantify the relationship between a dependent variable (often denoted as \\( $ Y $ \\)) and one or more independent variables (often denoted as \\( $ X $ \\)). These models can help predict the value of the dependent variable based on the values of the independent variables.\n",
    "\n",
    "For instance, linear regression is a type of regression model where the relationship between the dependent and independent variables is linear. The general equation for a simple linear regression (with one independent variable) is:\n",
    "\n",
    "$ Y = \\beta_0 + \\beta_1X + \\epsilon $\n",
    "\n",
    "where \\( $ \\beta_0 $ \\) is the intercept, \\( $ \\beta_1 $ \\) is the slope of the line, and \\( $ \\epsilon $ \\) represents the error term.\n",
    "\n",
    "## **Multicollinearity:**\n",
    "Multicollinearity refers to a situation in multiple regression where two or more independent variables are highly correlated. When this occurs, it can be difficult (or impossible) to isolate the effect of a single independent variable on the dependent variable. It can cause:\n",
    "1. Unstable coefficient estimates which can change significantly based on slight changes in the model.\n",
    "2. Reduced statistical significance of predictors, even if they're meaningful.\n",
    "3. Misleading interpretations of which variables are significant or relevant.\n",
    "\n",
    "## **Detecting Multicollinearity:**\n",
    "There are various methods to detect multicollinearity:\n",
    "\n",
    "1. **Variance Inflation Factor (VIF):** This is a popular metric. A VIF value greater than 10 (a common threshold) suggests high multicollinearity.\n",
    "\n",
    "$ \\text{VIF} = \\frac{1}{1 - R^2_j}  $\n",
    "\n",
    "where \\( $ R^2_j $ \\) is the \\( $ R^2 $ \\) value obtained by regressing the j-th independent variable against all other independent variables.\n",
    "\n",
    "2. **Correlation Matrices:** By examining the pairwise correlations between independent variables, one can identify pairs of variables that are highly correlated. A correlation value close to 1 or -1 indicates high multicollinearity.\n",
    "\n",
    "3. **Condition Index:** It involves eigenvalues of the scaled (not centered) predictor variables. A condition index above 30 indicates multicollinearity issues.\n",
    "\n",
    "4. **Eigenvalues and Eigenvectors:** When there's multicollinearity, several of the eigenvalues will be very small (close to zero).\n",
    "\n",
    "## **Dealing with Multicollinearity:**\n",
    "If multicollinearity is detected, here are potential remedies:\n",
    "1. **Remove Variables:** One can remove one of the correlated variables.\n",
    "2. **Combine Variables:** This can be done through techniques like Principal Component Analysis (PCA).\n",
    "3. **Increase Sample Size:** Sometimes, increasing the sample size can help.\n",
    "4. **Regularization Techniques:** Techniques like Ridge and Lasso regression can handle multicollinearity.\n",
    "\n",
    "It's important to note that multicollinearity is a problem only when we want to understand the effect of individual predictors on the response. If the goal is just prediction, multicollinearity might not be a concern."
   ],
   "metadata": {
    "id": "PDyXdQbyhxgA"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "12 λxyz.xz(yz(λfgx.fx(gx)))(λxy.xy)(λx.xx)(λx.xx)\n",
    "\n",
    "13 λxyz.xz(yz(λfgx.fx(gx)))(λxy.xy)(λx.xx)\n",
    "\n",
    "14 λxyz.xz(yz(λfgx.fx(gx)))(λxy.xy)(λx.xx)(λx.xx)\n",
    "\n",
    "11 λxyz.xz(yz(λfgx.fx(gx)))(λx.xx)(λx.xx)\n",
    "\n",
    "5  λxyz.xz(λxy.xy)(λx.xx)(λx.xx)\n"
   ],
   "metadata": {
    "id": "yhSdrquSfYz1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "id": "CwVL-5tLc90o"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "X = np.random.rand(4000, 5)\n",
    "y = np.random.rand(4000, 1)"
   ],
   "metadata": {
    "id": "wcjQfXyNc92w"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "scipy.stats.linregress(x, y=None, alternative='two-sided')[source]"
   ],
   "metadata": {
    "id": "qDQh0biIc97L"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "XvStFJ-nc99S"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "kJBJfbHBc9_d"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "gMpw-30rc-Bu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "d-aP2-04c-Ea"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
